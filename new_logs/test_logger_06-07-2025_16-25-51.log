2025-07-06 16:25:51,418 - INFO - init app
2025-07-06 16:25:52,997 - INFO - 
train: 8650
val: 2164
2025-07-06 16:25:53,037 - INFO - Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2025-07-06 16:25:53,075 - INFO - loading file tokenizer.model
2025-07-06 16:25:53,075 - INFO - loading file tokenizer.json
2025-07-06 16:25:53,075 - INFO - loading file added_tokens.json
2025-07-06 16:25:53,075 - INFO - loading file special_tokens_map.json
2025-07-06 16:25:53,075 - INFO - loading file tokenizer_config.json
2025-07-06 16:25:53,075 - INFO - loading file chat_template.jinja
2025-07-06 16:25:54,357 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\config.json
2025-07-06 16:25:54,385 - INFO - Model config Gemma3TextConfig {
  "architectures": [
    "Gemma3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": null,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "final_logit_softcapping": null,
  "head_dim": 256,
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 1152,
  "initializer_range": 0.02,
  "intermediate_size": 6912,
  "max_position_embeddings": 32768,
  "model_type": "gemma3_text",
  "num_attention_heads": 4,
  "num_hidden_layers": 26,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_local_base_freq": 10000,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": 512,
  "sliding_window_pattern": 6,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 262144
}

2025-07-06 16:25:56,252 - INFO - The device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' 
2025-07-06 16:25:56,252 - INFO - loading weights file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\model.safetensors
2025-07-06 16:25:56,266 - INFO - Instantiating Gemma3ForCausalLM model under default dtype torch.bfloat16.
2025-07-06 16:25:56,267 - INFO - Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "pad_token_id": 0
}

2025-07-06 16:26:01,756 - INFO - All model checkpoint weights were used when initializing Gemma3ForCausalLM.

2025-07-06 16:26:01,757 - INFO - All the weights of Gemma3ForCausalLM were initialized from the model checkpoint at ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma3ForCausalLM for predictions without further training.
2025-07-06 16:26:01,783 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\generation_config.json
2025-07-06 16:26:01,784 - INFO - Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "do_sample": true,
  "eos_token_id": [
    1,
    106
  ],
  "pad_token_id": 0,
  "top_k": 64,
  "top_p": 0.95
}

2025-07-06 16:26:01,795 - INFO - model load
2025-07-06 16:26:01,973 - INFO - ### answer:

2025-07-06 16:26:01,974 - INFO - PyTorch: setting up devices
2025-07-06 16:26:02,853 - INFO - Using auto half precision backend
2025-07-06 16:26:02,854 - WARNING - No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-07-06 16:26:02,854 - INFO - Start training for model gemma-3-1b-pt
2025-07-06 16:26:03,023 - INFO - ***** Running training *****
2025-07-06 16:26:03,023 - INFO -   Num examples = 24
2025-07-06 16:26:03,023 - INFO -   Num Epochs = 3
2025-07-06 16:26:03,023 - INFO -   Instantaneous batch size per device = 2
2025-07-06 16:26:03,023 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 4
2025-07-06 16:26:03,023 - INFO -   Gradient Accumulation steps = 2
2025-07-06 16:26:03,023 - INFO -   Total optimization steps = 18
2025-07-06 16:26:03,025 - INFO -   Number of trainable parameters = 1,810,432
2025-07-06 16:26:16,254 - INFO - 
Step 1: loss = 3.5673
Step 1: grad_norm = 3.8510217666625977
Step 1: learning_rate = 2e-05
Step 1: num_tokens = 2048.0
Step 1: mean_token_accuracy = 0.3694489449262619
Step 1: epoch = 0.16666666666666666
2025-07-06 16:26:16,295 - INFO - 
***** Running Evaluation *****
2025-07-06 16:26:16,296 - INFO -   Num examples = 5
2025-07-06 16:26:16,296 - INFO -   Batch size = 2
2025-07-06 16:26:45,182 - INFO - begin eval EM and F1 calculate func
2025-07-06 16:26:45,208 - INFO - <transformers.trainer_utils.EvalPrediction object at 0x0000016D7226A660>
2025-07-06 16:26:52,226 - INFO - 
--- Debugging compute_metrics_for_qa ---
2025-07-06 16:26:52,247 - INFO - Predictions (after argmax) shape: (5, 512)
2025-07-06 16:26:52,250 - INFO - Labels shape: (5, 512)
2025-07-06 16:26:52,251 - INFO - Answer Pattern: '### answer:
'
2025-07-06 16:26:52,254 - INFO - 
--- Sample 1 ---
2025-07-06 16:26:52,257 - INFO - Raw Prediction IDs (first 20): [1106, 669, 573, 107, 236770, 506, 2269, 3426, 1186, 57314, 236761, 108, 3890, 563, 577, 496, 11995, 4241, 699, 506]
2025-07-06 16:26:52,259 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:26:52,431 - INFO - Full Predicted Text:
import The for
1 the following below only clues.

 answer is be a integer match from the text. include a any additional explanation.

 the context is be answered using context,, then "No answer".

**
:
" to to we company team is the-


 the- is1106
74].
 on the use to a and accurate-term storage detection. when the presence of

clusion.
 in the.
 the to achieve these challenges, the researchers have a use of a novel- and,e on the--CNN) and with a tracking of the image toP- to. the to improve the of hand boxes. represent consistent constant. frames. and when the are occluded. one frames the frames.

### answer:
What does the researchers
 of the hand R-CNN model hand detection model? to the methods-of-the-art methods detection models?
### answer:
Based Answer

[
:
The the question using context only. The answer must be an exact quote from the context and not include any additional information.

 the question cannot be answered using context only, answer "No Answer"

### context:
Theieve a results in/
.t. the


 is lotome challenges with the the methods are used.

,, the2seq models are the one used in the
at-tions are challenges main: the1) the to and (2) the in training and and performance time..15,

 of the training methods2seq models are trained the-entropy loss, their training algorithm. gradient---ing.T ).1).

 the forcing, the the training process model, the model is the different: the input is and and and-1 and the former truthtruth target sequence, to generate the next output...

, the is a- generateethe gradient input in and.e., the1y. The, this the time, the model is outputs on the ground computed sequence, the teacher,, This a model-truth is is not available, the as situation is not to ensure the next token.

,, thehe, the model is is the from the teacher-, and in in testing test time, it is on the model output.

 introduces bias leads70] and a propagation, the-. the test time.

 of to mitigate this is is to use the ground-truth input from the.
2025-07-06 16:26:52,465 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
achieve great successes w.r.t. traditional methods, there aresome issues with how these models are trained. Generallyspeaking, seq2seq models like the ones used in NLP applica-tions face two issues: (1) exposure bias and (2) inconsistencybetween training time and test time measurements [70].Most of the popular seq2seq models are minimizing cross-entropy loss as their optimization objective via Teacher Forc-ing (Section III-B). In teacher forcing, during the training ofthe model, the decoder utilizes two inputs, the former decoderoutput state st−1 and the ground-truth input yt, to determine itscurrent output state st. Moreover, it employs them to createthe next token, i.e., ˆyt. However, at test time, the decoderfully relies on the previously created token from the modeldistribution. As the ground-truth data is not available, sucha step is necessary to predict the next action. Henceforth, intraining, the decoder input is coming from the ground truth,while, in the test phase, it relies on the previous prediction.This exposure bias [71] induces error growth through outputcreation at the test phase. One approach to remedy thisproblem is to remove the ground-truth dependency in training
2025-07-06 16:26:52,473 - INFO - Extracted Predicted Answer: 'Based Answer

[
:
The the question using context only. The answer must be an exact quote from the context and not include any additional information.

 the question cannot be answered using context only, answer "No Answer"

### context:
Theieve a results in/
.t. the


 is lotome challenges with the the methods are used.

,, the2seq models are the one used in the
at-tions are challenges main: the1) the to and (2) the in training and and performance time..15,

 of the training methods2seq models are trained the-entropy loss, their training algorithm. gradient---ing.T ).1).

 the forcing, the the training process model, the model is the different: the input is and and and-1 and the former truthtruth target sequence, to generate the next output...

, the is a- generateethe gradient input in and.e., the1y. The, this the time, the model is outputs on the ground computed sequence, the teacher,, This a model-truth is is not available, the as situation is not to ensure the next token.

,, thehe, the model is is the from the teacher-, and in in testing test time, it is on the model output.

 introduces bias leads70] and a propagation, the-. the test time.

 of to mitigate this is is to use the ground-truth input from the.'
2025-07-06 16:26:52,474 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
achieve great successes w.r.t. traditional methods, there aresome issues with how these models are trained. Generallyspeaking, seq2seq models like the ones used in NLP applica-tions face two issues: (1) exposure bias and (2) inconsistencybetween training time and test time measurements [70].Most of the popular seq2seq models are minimizing cross-entropy loss as their optimization objective via Teacher Forc-ing (Section III-B). In teacher forcing, during the training ofthe model, the decoder utilizes two inputs, the former decoderoutput state st−1 and the ground-truth input yt, to determine itscurrent output state st. Moreover, it employs them to createthe next token, i.e., ˆyt. However, at test time, the decoderfully relies on the previously created token from the modeldistribution. As the ground-truth data is not available, sucha step is necessary to predict the next action. Henceforth, intraining, the decoder input is coming from the ground truth,while, in the test phase, it relies on the previous prediction.This exposure bias [71] induces error growth through outputcreation at the test phase. One approach to remedy thisproblem is to remove the ground-truth dependency in training'
2025-07-06 16:26:52,506 - INFO - EM (this sample): 0.0
2025-07-06 16:26:52,513 - INFO - F1 (this sample): 0.5056818181818181
2025-07-06 16:26:52,513 - INFO - 
--- Sample 2 ---
2025-07-06 16:26:52,514 - INFO - Raw Prediction IDs (first 20): [2003, 5368, 44055, 580, 236761, 236761, 506, 531, 5296, 236772, 108, 236761, 236761, 108, 236761, 236761, 236770, 236842, 236810, 108]
2025-07-06 16:26:52,514 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:26:52,523 - INFO - Full Predicted Text:
by software relying on.. the to distribution-

..

..1]5

 the of method to minimize the..

 the sampling [ [ cannd getpardetermine the model. this-entropy loss. we we sample the model truthtruth. cross. cross is.

 model step is scheduled- is is is the we wendhe the donerst-, cross cross-entropy loss and we is then replaced and the-cross modelsable samples. as samples-GE and andSOR.

 is be the obstacle with the model and and the evaluation objective..

, weere been found that,, the models can be solvedledled with using the such the learning.7].].

 these these these problems-known problems, the learningThe





### is the of-time

 of we sampling is been reduced the2seq models performance? and then does these improvements handle to those methods- techniques?

### question:
What,

###[
:
1 the following. the..

 context should be  answer match from the text. must a any additional explanation.

 there context is be answered using the,, then "No context".

Context context:
"The.]]
.M, et. Wang, and. Zhang, et. Zhang, Z. Zhang, Z. Zhang, Y. Zhang, Z S.H. Chen,Aating a’ seq language translation: in2010, https### answer:
What to what methods, how has does does is the BERT improve neural machine translation improve? training?

### answer:
[ Answer
###[ context:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
[. Chenbash, et A. Mour, “Neural impact of neural-- a review of language modelsbots models,”  trends and and future future directions,”  Directions,  Future Future Directions,2 2, 2023).
 https023.178]

. At. Atail, and. Ast K, and. K.,, and J. A, “A largegpt for improve the responses conflicting information,”,” large intelligence,”  at httpsN:1839787, 2023.[

### question:
What is
2025-07-06 16:26:52,527 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
S. Atalla, and W. Mansoor, “The future of gpt: A taxonomy of existingchatgpt research, current challenges, and possible future directions,”Current Challenges, and Possible Future Directions (April 8, 2023) ,2023.[171] S. S. Sohail, D. Ø. Madsen, Y . Himeur, and M. Ashraf, “Using chatgptto navigate ambivalent and contradictory research findings on artificialintelligence,” Available at SSRN 4413913 , 2023.

### question:
What
2025-07-06 16:26:52,529 - INFO - Extracted Predicted Answer: '[ Answer
###[ context:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
[. Chenbash, et A. Mour, “Neural impact of neural-- a review of language modelsbots models,”  trends and and future future directions,”  Directions,  Future Future Directions,2 2, 2023).
 https023.178]

. At. Atail, and. Ast K, and. K.,, and J. A, “A largegpt for improve the responses conflicting information,”,” large intelligence,”  at httpsN:1839787, 2023.[

### question:
What is'
2025-07-06 16:26:52,530 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
S. Atalla, and W. Mansoor, “The future of gpt: A taxonomy of existingchatgpt research, current challenges, and possible future directions,”Current Challenges, and Possible Future Directions (April 8, 2023) ,2023.[171] S. S. Sohail, D. Ø. Madsen, Y . Himeur, and M. Ashraf, “Using chatgptto navigate ambivalent and contradictory research findings on artificialintelligence,” Available at SSRN 4413913 , 2023.

### question:
What'
2025-07-06 16:26:52,531 - INFO - EM (this sample): 0.0
2025-07-06 16:26:52,532 - INFO - F1 (this sample): 0.5647058823529411
2025-07-06 16:26:52,532 - INFO - 
--- Sample 3 ---
2025-07-06 16:26:52,532 - INFO - Raw Prediction IDs (first 20): [659, 659, 108, 108, 236772, 236761, 506, 236772, 236772, 236772, 506, 6549, 236761, 496, 236761, 532, 532, 1388, 528, 14168]
2025-07-06 16:26:52,533 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:26:52,537 - INFO - Full Predicted Text:
 are are



-. the--- the ideas. a. and and well in researchers W' Ar, and.,,, and others,



:
###,.

[

:
1 the following in the..

 context should be a exact match from the text. must a any additional explanation.

 the context is be answered using the,, then "I answer".

Context Question:
The. The, aThe networks are deep entity recognition” in preprint :2603.03679, 2016.https]]

.M. Chen, C. A, “A entity recognition,” a context,”based,”,” Proceedings preprint arXiv:1601.17781, 2015.[10]

.S, A. Kune, “A novel of named advances in named entity recognition,” deep learning,”,” IEEE preprint arXiv:1600.01815, 2019.[21] C. A, “. K, and. Chen, and Y. Dyer, “Neural novel on named learning for named entity recognition,” arXiv Transactions on Pattern and Data Engineering, 2020.[22] C. P, Y. Zhang, “Deep-to-end named for named role labeling,” a neural networks,” arXiv Proceedings of the 2thrd annual Meeting of the Association for Computational Linguistics, the International3 International Symposium Conference on Artificial Language Processing,2,1, Tables Title), . 1, . 1-33–1136, 2018.[

### question:
What is language are mentioned in the and Nichols? their work? named entity recognition? bidirectional LSTM-CNNs?

### answer:
J Answer

[
:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
CCH, al al. “ forANCES , CONEP LEARNING,1,. 1.1- are in the study include
 authors used a variety of the performance of the art in learning for NLP techniques. the variety text dataset task.
 specifically, Chiu the19],] Chiu new approachorder approach-F was to introduced to the parserXactord LSTM115]
2025-07-06 16:26:52,541 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9Fig. 8. NLP tasks investigated in this study.the authors provide a comparison on the state of the art deeplearning based parsing methods on a clinical text parsing task.More recently, in [104], a second-order TreeCRF extensionwas added to the biafﬁne [105
2025-07-06 16:26:52,542 - INFO - Extracted Predicted Answer: 'J Answer

[
:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
CCH, al al. “ forANCES , CONEP LEARNING,1,. 1.1- are in the study include
 authors used a variety of the performance of the art in learning for NLP techniques. the variety text dataset task.
 specifically, Chiu the19],] Chiu new approachorder approach-F was to introduced to the parserXactord LSTM115]'
2025-07-06 16:26:52,542 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9Fig. 8. NLP tasks investigated in this study.the authors provide a comparison on the state of the art deeplearning based parsing methods on a clinical text parsing task.More recently, in [104], a second-order TreeCRF extensionwas added to the biafﬁne [105'
2025-07-06 16:26:52,544 - INFO - EM (this sample): 0.0
2025-07-06 16:26:52,544 - INFO - F1 (this sample): 0.5503355704697986
2025-07-06 16:26:52,567 - INFO - 
--- Batch Metrics ---
2025-07-06 16:26:52,567 - INFO - Average EM: 0.2000
2025-07-06 16:26:52,567 - INFO - Average F1: 0.6428
2025-07-06 16:26:52,567 - INFO - --- End compute_metrics_for_qa ---
2025-07-06 16:26:52,631 - INFO - 
Step 1: eval_loss = 2.499985456466675
Step 1: eval_em = 0.2
Step 1: eval_f1 = 0.642765343856084
Step 1: eval_runtime = 36.3223
Step 1: eval_samples_per_second = 0.138
Step 1: eval_steps_per_second = 0.083
Step 1: eval_num_tokens = 2048.0
Step 1: eval_mean_token_accuracy = 0.7021577556927999
Step 1: epoch = 0.16666666666666666
2025-07-06 16:26:52,838 - INFO - Saving model checkpoint to ./saved_models/train__06-07-2025_16-26-01__gemma-3-1b-pt\checkpoint-1
2025-07-06 16:26:53,455 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\config.json
2025-07-06 16:26:53,458 - INFO - Model config Gemma3TextConfig {
  "architectures": [
    "Gemma3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": null,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "final_logit_softcapping": null,
  "head_dim": 256,
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 1152,
  "initializer_range": 0.02,
  "intermediate_size": 6912,
  "max_position_embeddings": 32768,
  "model_type": "gemma3_text",
  "num_attention_heads": 4,
  "num_hidden_layers": 26,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_local_base_freq": 10000,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": 512,
  "sliding_window_pattern": 6,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 262144
}

2025-07-06 16:26:53,569 - INFO - tokenizer config file saved in ./saved_models/train__06-07-2025_16-26-01__gemma-3-1b-pt\checkpoint-1\tokenizer_config.json
2025-07-06 16:26:53,570 - INFO - Special tokens file saved in ./saved_models/train__06-07-2025_16-26-01__gemma-3-1b-pt\checkpoint-1\special_tokens_map.json
2025-07-06 16:27:03,747 - INFO - 
Step 2: loss = 3.8608
Step 2: grad_norm = 3.712554454803467
Step 2: learning_rate = 1.888888888888889e-05
Step 2: num_tokens = 4096.0
Step 2: mean_token_accuracy = 0.3896653801202774
Step 2: epoch = 0.3333333333333333
2025-07-06 16:27:03,761 - INFO - 
***** Running Evaluation *****
2025-07-06 16:27:03,761 - INFO -   Num examples = 5
2025-07-06 16:27:03,761 - INFO -   Batch size = 2
