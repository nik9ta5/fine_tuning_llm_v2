2025-07-06 16:07:44,237 - INFO - init app
2025-07-06 16:07:45,237 - INFO - 
train: 8650
val: 2164
2025-07-06 16:07:45,248 - INFO - Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2025-07-06 16:07:45,271 - INFO - loading file tokenizer.model
2025-07-06 16:07:45,271 - INFO - loading file tokenizer.json
2025-07-06 16:07:45,271 - INFO - loading file added_tokens.json
2025-07-06 16:07:45,271 - INFO - loading file special_tokens_map.json
2025-07-06 16:07:45,271 - INFO - loading file tokenizer_config.json
2025-07-06 16:07:45,271 - INFO - loading file chat_template.jinja
2025-07-06 16:07:46,664 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\config.json
2025-07-06 16:07:46,667 - INFO - Model config Gemma3TextConfig {
  "architectures": [
    "Gemma3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": null,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "final_logit_softcapping": null,
  "head_dim": 256,
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 1152,
  "initializer_range": 0.02,
  "intermediate_size": 6912,
  "max_position_embeddings": 32768,
  "model_type": "gemma3_text",
  "num_attention_heads": 4,
  "num_hidden_layers": 26,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_local_base_freq": 10000,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": 512,
  "sliding_window_pattern": 6,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 262144
}

2025-07-06 16:07:46,800 - INFO - The device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' 
2025-07-06 16:07:46,800 - INFO - loading weights file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\model.safetensors
2025-07-06 16:07:46,821 - INFO - Instantiating Gemma3ForCausalLM model under default dtype torch.bfloat16.
2025-07-06 16:07:46,823 - INFO - Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "pad_token_id": 0
}

2025-07-06 16:07:52,742 - INFO - All model checkpoint weights were used when initializing Gemma3ForCausalLM.

2025-07-06 16:07:52,749 - INFO - All the weights of Gemma3ForCausalLM were initialized from the model checkpoint at ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma3ForCausalLM for predictions without further training.
2025-07-06 16:07:52,758 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\generation_config.json
2025-07-06 16:07:52,760 - INFO - Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "do_sample": true,
  "eos_token_id": [
    1,
    106
  ],
  "pad_token_id": 0,
  "top_k": 64,
  "top_p": 0.95
}

2025-07-06 16:07:52,792 - INFO - model load
2025-07-06 16:07:53,019 - INFO - ### answer:

2025-07-06 16:07:53,026 - INFO - PyTorch: setting up devices
2025-07-06 16:07:54,019 - INFO - Using auto half precision backend
2025-07-06 16:07:54,020 - WARNING - No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-07-06 16:07:54,021 - INFO - Start training for model gemma-3-1b-pt
2025-07-06 16:07:54,455 - INFO - ***** Running training *****
2025-07-06 16:07:54,456 - INFO -   Num examples = 24
2025-07-06 16:07:54,456 - INFO -   Num Epochs = 3
2025-07-06 16:07:54,456 - INFO -   Instantaneous batch size per device = 2
2025-07-06 16:07:54,456 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 4
2025-07-06 16:07:54,456 - INFO -   Gradient Accumulation steps = 2
2025-07-06 16:07:54,456 - INFO -   Total optimization steps = 18
2025-07-06 16:07:54,458 - INFO -   Number of trainable parameters = 1,810,432
2025-07-06 16:08:08,216 - INFO - 
Step 1: loss = 3.5673
Step 1: grad_norm = 3.420790195465088
Step 1: learning_rate = 2e-05
Step 1: num_tokens = 2048.0
Step 1: mean_token_accuracy = 0.3694489449262619
Step 1: epoch = 0.16666666666666666
2025-07-06 16:08:08,259 - INFO - 
***** Running Evaluation *****
2025-07-06 16:08:08,260 - INFO -   Num examples = 5
2025-07-06 16:08:08,260 - INFO -   Batch size = 2
2025-07-06 16:09:05,944 - INFO - begin eval EM and F1 calculate func
2025-07-06 16:09:06,178 - INFO - <transformers.trainer_utils.EvalPrediction object at 0x000001F88A6FE660>
2025-07-06 16:09:39,213 - INFO - 
--- Debugging compute_metrics_for_qa ---
2025-07-06 16:09:39,240 - INFO - Predictions (after argmax) shape: (5, 512)
2025-07-06 16:09:39,243 - INFO - Labels shape: (5, 512)
2025-07-06 16:09:39,243 - INFO - Answer Pattern: '### answer:
'
2025-07-06 16:09:39,246 - INFO - 
--- Sample 1 ---
2025-07-06 16:09:39,249 - INFO - Raw Prediction IDs (first 20): [1106, 568, 573, 107, 236770, 506, 2269, 236761, 1186, 236761, 236761, 108, 3890, 563, 577, 528, 11995, 4241, 699, 506]
2025-07-06 16:09:39,250 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:09:39,421 - INFO - Full Predicted Text:
import ( for
1 the following. only..

 answer is be in integer match from the text.. a a explanation explanation.

 the context is be answered using context,, then withCannot answer".

** Context:
" the to we company team on the-


 the- is1108
74].
 on the use for a and accurate-range
 detection. when the presence of

clusion.
 in the.
 the to achieve these challenges, the researchers developed a use of a new-
.e on the--CNN) and with a tracking of the image toPI- to. the to improve the of hand boxes. represent consistent constant. frames.
 when the are occluded. the frames the frames.

### answer:
What does the researchers
 of the hand R-CNN model hand detection model? to the methods-of-the-art methods detection models?
### answer:
Based Answer

[
:
The the question using context only. The answer must be an exact quote from the context and not include any additional information.

 the question cannot be answered using context only, answer "No Answer"

### context:
Theieving a results in/
.t. the
,
 is numberome challenges with the the methods are used.
,, the2seq models are the one used in the
at-tions are challenges main: the1) the to and (2) the in the and and performance time..15,

 of the models methods2seq models are trained the-entropy loss, the primary algorithm. gradient---ing.or .1).

 the forcing, the the training process model, the model is the different: the input is and and and-1 and the former truthtruth target,, to generate the next output...

, the is a- generateethe model state. and.e., the1y. The, this the time, the model is uses on the ground computed output, the teacher,, This a model-truth is is not available, the as situation can not to ensure the next token.

,, thehe, the model is is the from the model-, and in in testing test time, it is on the model output.

 introduces bias can70] and a propagation, the-. the test time.

 of to mitigate this is is to use the previous-truth input from the.
2025-07-06 16:09:39,474 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
achieve great successes w.r.t. traditional methods, there aresome issues with how these models are trained. Generallyspeaking, seq2seq models like the ones used in NLP applica-tions face two issues: (1) exposure bias and (2) inconsistencybetween training time and test time measurements [70].Most of the popular seq2seq models are minimizing cross-entropy loss as their optimization objective via Teacher Forc-ing (Section III-B). In teacher forcing, during the training ofthe model, the decoder utilizes two inputs, the former decoderoutput state st−1 and the ground-truth input yt, to determine itscurrent output state st. Moreover, it employs them to createthe next token, i.e., ˆyt. However, at test time, the decoderfully relies on the previously created token from the modeldistribution. As the ground-truth data is not available, sucha step is necessary to predict the next action. Henceforth, intraining, the decoder input is coming from the ground truth,while, in the test phase, it relies on the previous prediction.This exposure bias [71] induces error growth through outputcreation at the test phase. One approach to remedy thisproblem is to remove the ground-truth dependency in training
2025-07-06 16:09:39,478 - INFO - Extracted Predicted Answer: 'Based Answer

[
:
The the question using context only. The answer must be an exact quote from the context and not include any additional information.

 the question cannot be answered using context only, answer "No Answer"

### context:
Theieving a results in/
.t. the
,
 is numberome challenges with the the methods are used.
,, the2seq models are the one used in the
at-tions are challenges main: the1) the to and (2) the in the and and performance time..15,

 of the models methods2seq models are trained the-entropy loss, the primary algorithm. gradient---ing.or .1).

 the forcing, the the training process model, the model is the different: the input is and and and-1 and the former truthtruth target,, to generate the next output...

, the is a- generateethe model state. and.e., the1y. The, this the time, the model is uses on the ground computed output, the teacher,, This a model-truth is is not available, the as situation can not to ensure the next token.

,, thehe, the model is is the from the model-, and in in testing test time, it is on the model output.

 introduces bias can70] and a propagation, the-. the test time.

 of to mitigate this is is to use the previous-truth input from the.'
2025-07-06 16:09:39,480 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
achieve great successes w.r.t. traditional methods, there aresome issues with how these models are trained. Generallyspeaking, seq2seq models like the ones used in NLP applica-tions face two issues: (1) exposure bias and (2) inconsistencybetween training time and test time measurements [70].Most of the popular seq2seq models are minimizing cross-entropy loss as their optimization objective via Teacher Forc-ing (Section III-B). In teacher forcing, during the training ofthe model, the decoder utilizes two inputs, the former decoderoutput state st−1 and the ground-truth input yt, to determine itscurrent output state st. Moreover, it employs them to createthe next token, i.e., ˆyt. However, at test time, the decoderfully relies on the previously created token from the modeldistribution. As the ground-truth data is not available, sucha step is necessary to predict the next action. Henceforth, intraining, the decoder input is coming from the ground truth,while, in the test phase, it relies on the previous prediction.This exposure bias [71] induces error growth through outputcreation at the test phase. One approach to remedy thisproblem is to remove the ground-truth dependency in training'
2025-07-06 16:09:39,515 - INFO - EM (this sample): 0.0
2025-07-06 16:09:39,524 - INFO - F1 (this sample): 0.49425287356321834
2025-07-06 16:09:39,525 - INFO - 
--- Sample 2 ---
2025-07-06 16:09:39,525 - INFO - Raw Prediction IDs (first 20): [2003, 2174, 44055, 506, 236761, 236761, 30016, 506, 5296, 236772, 236772, 236761, 236761, 108, 236761, 531, 20955, 236842, 236810, 531]
2025-07-06 16:09:39,526 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:09:39,545 - INFO - Full Predicted Text:
by allow relying the.. distribute the distribution--..

. tocross]5 to the to method to minimize the..

 sampling sampling [ [ cann encounterpardetermine this setback. this-entropy loss. we apply adjust the model truthtruth. cross. model..

 scheduled step is the- is is is to we wendhe the donerst-, cross cross-entropy loss and we is then replaced with the-cross-able samples and as samples andGE and andSor and

 is be the obstacle with the model and and the evaluation...

, weere been found that,, the models are be solvedledled with using the such the learning.7].].

 the these these problems-known problems, reinforcement learningThe





### is the of-time- of we sampling is been reduced the2seq models?? and then does these applications handle to the methods- techniques?

### question:
What,:###[

:
1 the following. the..

 question should be a answer match from the text. not a any additional explanation.

 there context is be answered using the,, then "I answer".

Context context:
"The.]]
.M, et. Wang, and. Zhang, and. Zhang, Z. Zhang, Y. Zhang, Y. Zhang, Z M.H. Wang,Aating a for seq machine translation: in2010, https### answer:
What to what methods, how has has does did the bert improve neural machine translation improve? training?

### answer:
[ Answer
###[ context:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
[. Zhangbash, J J. Lour, “Neural impact of neuralpt- a review of language modelsbots models,”  trends and and future future directions,”  Directions,  Future Future Directions,2 2, 2023).

 https023.178]

. At. Atail, and. Aster K, and. K.,, and J. A, “The agpt to improve the responses conflicting information,”,” large intelligence,”arXiv at httpsN,18799.,, 2023.

### question:
What is
2025-07-06 16:09:39,552 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
S. Atalla, and W. Mansoor, “The future of gpt: A taxonomy of existingchatgpt research, current challenges, and possible future directions,”Current Challenges, and Possible Future Directions (April 8, 2023) ,2023.[171] S. S. Sohail, D. Ø. Madsen, Y . Himeur, and M. Ashraf, “Using chatgptto navigate ambivalent and contradictory research findings on artificialintelligence,” Available at SSRN 4413913 , 2023.

### question:
What
2025-07-06 16:09:39,554 - INFO - Extracted Predicted Answer: '[ Answer
###[ context:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
[. Zhangbash, J J. Lour, “Neural impact of neuralpt- a review of language modelsbots models,”  trends and and future future directions,”  Directions,  Future Future Directions,2 2, 2023).

 https023.178]

. At. Atail, and. Aster K, and. K.,, and J. A, “The agpt to improve the responses conflicting information,”,” large intelligence,”arXiv at httpsN,18799.,, 2023.

### question:
What is'
2025-07-06 16:09:39,554 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
S. Atalla, and W. Mansoor, “The future of gpt: A taxonomy of existingchatgpt research, current challenges, and possible future directions,”Current Challenges, and Possible Future Directions (April 8, 2023) ,2023.[171] S. S. Sohail, D. Ø. Madsen, Y . Himeur, and M. Ashraf, “Using chatgptto navigate ambivalent and contradictory research findings on artificialintelligence,” Available at SSRN 4413913 , 2023.

### question:
What'
2025-07-06 16:09:39,558 - INFO - EM (this sample): 0.0
2025-07-06 16:09:39,560 - INFO - F1 (this sample): 0.5614035087719298
2025-07-06 16:09:39,560 - INFO - 
--- Sample 3 ---
2025-07-06 16:09:39,560 - INFO - Raw Prediction IDs (first 20): [659, 659, 108, 108, 236772, 236761, 506, 672, 236772, 236772, 506, 6549, 236761, 506, 236761, 236761, 532, 496, 528, 14168]
2025-07-06 16:09:39,560 - INFO - Raw Label IDs (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
2025-07-06 16:09:39,570 - INFO - Full Predicted Text:
 are are



-. the this-- the ideas. the.. and a in researchers

 Shah Ar, and.,,, and M.?This

:

###,.

[

:
1 in following in the..

 context should be in exact match from the text. must a any additional explanation.

 the context is be answered using the,, then "I answer".

Context
:
Theaffeine1, aThe networks are deep entity recognition” in preprint :2703.03989, 2016.https]]

.M. Chen, C. J, “Named entity recognition,” a LSTM,”based,”,” Proceedings preprint arXiv:1601.27931, 2015.[10]

. K, A. K,, “A novel of named advances in named entity recognition,” the learning,”,” IEEE preprint arXiv:1602.01803, 2019.[21] C. M, “. M, and. M, and Y. Dyer, “Neural novel on named learning for named entity recognition,” arXiv Transactions on Pattern and Data Engineering, 2020.[22] C. P, Y. Zhang, “Deep-to-end named of named role labeling,” deep neural networks,” arXiv Proceedings of the 2thrd annual Meeting of the Association for Computational Linguistics, the International3 International Symposium Conference on Artificial Language Processing,K,1,  Title),ACM. 1, . 1-3––1136, 2019.[

### question:
What is language are mentioned in the and Nichols? their work? named entity recognition? bidirectional LSTM-CNNs?

### answer:
No Answer

[
:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
CCH, al al. “ forANCES,, LEP LEARNING,2th. 1.1- are in the study.
 use used a variety of the performance- the art in learning techniques NLP and. various variety text dataset task.
 specifically, Chiu the19],] Chiu new studyorder approach-F was was introduced to the parserxided LSTM115]
2025-07-06 16:09:39,583 - INFO - Full Golden Text (filtered -100):
No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9Fig. 8. NLP tasks investigated in this study.the authors provide a comparison on the state of the art deeplearning based parsing methods on a clinical text parsing task.More recently, in [104], a second-order TreeCRF extensionwas added to the biafﬁne [105
2025-07-06 16:09:39,584 - INFO - Extracted Predicted Answer: 'No Answer

[
:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
CCH, al al. “ forANCES,, LEP LEARNING,2th. 1.1- are in the study.
 use used a variety of the performance- the art in learning techniques NLP and. various variety text dataset task.
 specifically, Chiu the19],] Chiu new studyorder approach-F was was introduced to the parserxided LSTM115]'
2025-07-06 16:09:39,584 - INFO - Extracted Golden Answer:  'No Answer### instructions:
Answer the question using context only. The answer must be an exact quote from the context and not include any additional information. If the question cannot be answered using context only, answer "No Answer"

### context:
TORFI et. al., NLP ADV ANCEMENTS BY DEEP LEARNING 9Fig. 8. NLP tasks investigated in this study.the authors provide a comparison on the state of the art deeplearning based parsing methods on a clinical text parsing task.More recently, in [104], a second-order TreeCRF extensionwas added to the biafﬁne [105'
2025-07-06 16:09:39,590 - INFO - EM (this sample): 0.0
2025-07-06 16:09:39,593 - INFO - F1 (this sample): 0.5540540540540541
2025-07-06 16:09:39,664 - INFO - 
--- Batch Metrics ---
2025-07-06 16:09:39,665 - INFO - Average EM: 0.0000
2025-07-06 16:09:39,665 - INFO - Average F1: 0.5756
2025-07-06 16:09:39,665 - INFO - --- End compute_metrics_for_qa ---
2025-07-06 16:09:39,763 - INFO - 
Step 1: eval_loss = 2.7838337421417236
Step 1: eval_em = 0.0
Step 1: eval_f1 = 0.575555140890894
Step 1: eval_runtime = 91.4846
Step 1: eval_samples_per_second = 0.055
Step 1: eval_steps_per_second = 0.033
Step 1: eval_num_tokens = 2048.0
Step 1: eval_mean_token_accuracy = 0.5360215504964193
Step 1: epoch = 0.16666666666666666
2025-07-06 16:09:39,961 - INFO - Saving model checkpoint to ./saved_models/train__06-07-2025_16-07-53__gemma-3-1b-pt\checkpoint-1
2025-07-06 16:09:40,298 - INFO - loading configuration file ../../../all_language_models/models--google--gemma-3-1b-it/snapshots/dcc83ea841ab6100d6b47a070329e1ba4cf78752\config.json
2025-07-06 16:09:40,304 - INFO - Model config Gemma3TextConfig {
  "architectures": [
    "Gemma3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": null,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": [
    1,
    106
  ],
  "final_logit_softcapping": null,
  "head_dim": 256,
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 1152,
  "initializer_range": 0.02,
  "intermediate_size": 6912,
  "max_position_embeddings": 32768,
  "model_type": "gemma3_text",
  "num_attention_heads": 4,
  "num_hidden_layers": 26,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_local_base_freq": 10000,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": 512,
  "sliding_window_pattern": 6,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 262144
}

