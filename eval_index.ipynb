{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b7ca64-3c68-4183-b37e-5ca8d981d926",
   "metadata": {},
   "source": [
    "### Временный блокнот для оценки обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c836c7-0bc5-441a-a54c-bddae4d590a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общие\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Для оценки\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Для логера\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.seed = RANDOM_STATE\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96f94a5-39d3-4d02-8f9f-ee75afde4f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'name': 'google/gemma-3-4b-it',\n",
       "  'model_name_log': 'gemma-3-4b-it',\n",
       "  'cache_dir': '../ft_v1/models_cache',\n",
       "  'max_length': 512,\n",
       "  'max_new_tokens': 32,\n",
       "  'tokenizer': {'padding_size': 'left'}},\n",
       " 'lora': {'r': 16,\n",
       "  'lora_alpha': 32,\n",
       "  'lora_dropout': 0.05,\n",
       "  'target_modules': ['q_proj', 'o_proj', 'v_proj', 'k_proj']},\n",
       " 'inference': {'temp': 0.7},\n",
       " 'train': {'model_save_dir': './saved_models',\n",
       "  'epochs': 3,\n",
       "  'train_batch': 8,\n",
       "  'val_batch': 8,\n",
       "  'test_batch': 8,\n",
       "  'grad_accum': 8,\n",
       "  'eval_step': 5000,\n",
       "  'save_step': 5000,\n",
       "  'torch_empty_cache_steps': 8,\n",
       "  'log_step': 500,\n",
       "  'lr': '3e-4',\n",
       "  'weight_decay': 0.01},\n",
       " 'logs': {'dir': './logs_all'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'./config.yaml', 'r') as file:\n",
    "    CONFIG = yaml.safe_load(file)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f83a045-bced-4f46-a9ca-ca9a68773101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rajpurkar/squad_v2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'squad_v2' at ../ft_v1/cdatasets/rajpurkar___squad_v2/squad_v2/0.0.0/3ffb306f725f7d2ce8394bc1873b24868140c412 (last modified on Sat Jun  7 18:35:48 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size train part: 130319\n",
      "Size validation part: 11873\n"
     ]
    }
   ],
   "source": [
    "#Пути до моделей\n",
    "path2Gemma3_1B = '../ndora/cache_dir'\n",
    "path2Gemma3_4B = '../ft_v1/models_cache/'\n",
    "path2Llama31_8B = '../ft_v1/models_cache/'\n",
    "\n",
    "model_name_Gemma_1 = 'google/gemma-3-1b-it'\n",
    "model_name_Gemma_4 = 'google/gemma-3-4b-it'\n",
    "\n",
    "model_name_llama_31 = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "#Пути до датасетов\n",
    "SQuAD_2_path = '../ft_v1/cdatasets/' # Путь до датасета SQuAD 2.0\n",
    "\n",
    "\n",
    "datasetSQUAD2 = load_dataset(\"rajpurkar/squad_v2\", cache_dir=SQuAD_2_path)\n",
    "\n",
    "train_dataset = datasetSQUAD2['train']\n",
    "val_dataset = datasetSQUAD2['validation']\n",
    "\n",
    "print(f\"Size train part: {len(train_dataset)}\")\n",
    "print(f\"Size validation part: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4e4499-5fef-4ed7-88c5-5468241cc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Общая инструкция для промпта\n",
    "INSTRUCTION = 'You are a strict AI assistant designed to answer a question based on context. Answer verbatim from the context. Do not guess or formulate, answer as it is in the context. If there is no answer in the context, answer \"No answer\".'\n",
    "\n",
    "# Промпт, будет использоваться при обучении и оценки модели\n",
    "def prompt_template(example):\n",
    "    global INSTRUCTION\n",
    "    return f\"{INSTRUCTION}\\nContext:{example['Context']}\\nQuestion:{example['Question']}\\nAnswer:{example['Answer']}\"\n",
    "\n",
    "def prompt_template(context, question, answer):\n",
    "    global INSTRUCTION\n",
    "    return f\"{INSTRUCTION}\\nContext:{context}\\nQuestion:{question}\\nAnswer:{answer}\"\n",
    "\n",
    "def prompt_template_test(context, question):\n",
    "    global INSTRUCTION\n",
    "    return f\"{INSTRUCTION}\\nContext:{context}\\nQuestion:{question}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "092935c9-6003-487a-bba8-91d8759b7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_answer_ft(batch):\n",
    "    \"\"\" Функция для формирования промпротов для train\n",
    "    (ОСОБЕННОСТЬ: SQuAD формат)\n",
    "    return (prompt, answer) - кортеж\n",
    "    \"\"\"\n",
    "    answers = [\n",
    "        item['text'][0] if item['text'] else \"\"\n",
    "        for item in batch['answers']\n",
    "    ]\n",
    "    prompts = [\n",
    "        prompt_template(context, question, answer) \n",
    "        for context, question, answer in zip(batch['context'], batch['question'], answers)\n",
    "    ]\n",
    "    return {\n",
    "        \"prompt\": prompts, \n",
    "        \"answer\": answers\n",
    "    }\n",
    "\n",
    "\n",
    "def prompt_answer_fn_test(batch):\n",
    "    \"\"\" Функция для формирования промпротов для test or val\n",
    "    (ОСОБЕННОСТЬ: SQuAD формат)\n",
    "    return (prompt, answer) - кортеж\n",
    "    \"\"\"\n",
    "    answers = [\n",
    "        item['text'][0] if item['text'] else \"\"\n",
    "        for item in batch['answers']\n",
    "    ]\n",
    "    prompts = [\n",
    "        prompt_template_test(context, question) \n",
    "        for context, question in zip(batch['context'], batch['question'])\n",
    "    ]\n",
    "    return {\n",
    "        \"prompt\": prompts, \n",
    "        \"answer\": answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec829a0-ae7b-48f9-9d11-e8736f93eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text(batch, tokenizer, answer_pattern = \"Answer:\", max_len_seq = 1024):\n",
    "    \"\"\" Функция для токенизации промптов\n",
    "\n",
    "    return: input_ids, attention_mask, labels, answers\n",
    "    \"\"\"\n",
    "    prompts = batch['prompt']\n",
    "    answers = batch['answer']\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_len_seq,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    #Для лейблов - клонируем тензор токенизированного промпта\n",
    "    labels = encodings['input_ids'].clone()\n",
    "    for i in range(len(prompts)):\n",
    "        #Находим индекс слова в промпте, которое считается началом ответа для языковой модели\n",
    "        answer_start = prompts[i].find(answer_pattern)\n",
    "        \n",
    "        #Определяем начало слова ответа в токенизированном формате (длинна тензора отличается от длинны промпта как строки)\n",
    "        answer_start_id = len(prompts[i])\n",
    "        if answer_start != -1:\n",
    "            answer_start_id = tokenizer(prompts[i][:answer_start], return_tensors=\"pt\")['input_ids'].size(1)\n",
    "\n",
    "        #Сколько токенов паддинга\n",
    "        padding_tokens = max_len_seq - encodings['attention_mask'][i].sum().item() \n",
    "\n",
    "        #Что не является ответом - устанавливаем -100 (defualt в PyTorch and HF)\n",
    "        labels[i, :padding_tokens + answer_start_id] = -100\n",
    "\n",
    "    encodings['labels'] = labels\n",
    "    encodings['answer'] = answers\n",
    "    return encodings # input_ids, attention_mask, labels, answers\n",
    "    \n",
    "\n",
    "def dataset_preprocess(dataset, tokenizer, answer_pattern = \"Answer:\", max_len_seq = 1024, eval=False):\n",
    "    \"\"\"Функция для преобразования датасета\"\"\"\n",
    "    if eval:\n",
    "        dataset = dataset.map(\n",
    "            prompt_answer_fn_test, #Функция, которая применяется ко всем строкам\n",
    "            batched=True,        #Использовать батчинг\n",
    "            num_proc=1,         #Количество процессов\n",
    "            remove_columns=dataset.column_names,  #Удаляем исходные колонки\n",
    "        ) #prompt, answer - NO TOKENIZE\n",
    "    else:\n",
    "        dataset = dataset.map(\n",
    "            prompt_answer_ft, #Функция, которая применяется ко всем строкам\n",
    "            batched=True,        #Использовать батчинг\n",
    "            num_proc=1,         #Количество процессов\n",
    "            remove_columns=dataset.column_names,  #Удаляем исходные колонки\n",
    "        ) #prompt, answer - NO TOKENIZE\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenized_text(x, tokenizer, answer_pattern=answer_pattern, max_len_seq=max_len_seq), #Функция, которая применяется ко всем строкам, #Функция, которая применяется ко всем строкам\n",
    "        batched=True,        #Использовать батчинг\n",
    "        num_proc=1,         #Количество процессов\n",
    "        remove_columns=dataset.column_names,  #Удаляем исходные колонки\n",
    "    )\n",
    "    return dataset #На данном этапе в датасетах - input_ids, attention_mask, labels, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "696f69be-ba9e-42e2-bea6-9dde3d2fe2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    return {\"input_ids\": torch.stack(input_ids),\"attention_mask\": torch.stack(attention_mask),\"labels\": torch.stack(labels),\"answer\": answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85e0fb7e-28e8-4352-9dd1-9feb9a3ad90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "loading file tokenizer.model from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/tokenizer.model\n",
      "loading file tokenizer.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/tokenizer.json\n",
      "loading file added_tokens.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/config.json\n",
      "text_config is None, using default Gemma3TextConfig text config.\n",
      "vision_config is None, using default SiglipVisionConfig vision config.\n",
      "Model config Gemma3Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"boi_token_index\": 255999,\n",
      "  \"eoi_token_index\": 256000,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"image_token_index\": 262144,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"mm_tokens_per_image\": 256,\n",
      "  \"model_type\": \"gemma3\",\n",
      "  \"text_config\": {\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"attn_logit_softcapping\": null,\n",
      "    \"cache_implementation\": \"hybrid\",\n",
      "    \"final_logit_softcapping\": null,\n",
      "    \"head_dim\": 256,\n",
      "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 2560,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 10240,\n",
      "    \"max_position_embeddings\": 131072,\n",
      "    \"model_type\": \"gemma3_text\",\n",
      "    \"num_attention_heads\": 8,\n",
      "    \"num_hidden_layers\": 34,\n",
      "    \"num_key_value_heads\": 4,\n",
      "    \"query_pre_attn_scalar\": 256,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_local_base_freq\": 10000.0,\n",
      "    \"rope_scaling\": {\n",
      "      \"factor\": 8.0,\n",
      "      \"rope_type\": \"linear\"\n",
      "    },\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": 1024,\n",
      "    \"sliding_window_pattern\": 6,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 262208\n",
      "  },\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"vision_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 1152,\n",
      "    \"image_size\": 896,\n",
      "    \"intermediate_size\": 4304,\n",
      "    \"layer_norm_eps\": 1e-06,\n",
      "    \"model_type\": \"siglip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 27,\n",
      "    \"patch_size\": 14,\n",
      "    \"vision_use_head\": false\n",
      "  }\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/model.safetensors.index.json\n",
      "Instantiating Gemma3ForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ]\n",
      "}\n",
      "\n",
      "Instantiating SiglipVisionModel model under default dtype torch.bfloat16.\n",
      "Instantiating Gemma3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72887e7db144299dad4f842447ad10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Gemma3ForConditionalGeneration.\n",
      "\n",
      "All the weights of Gemma3ForConditionalGeneration were initialized from the model checkpoint at google/gemma-3-4b-it.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma3ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at ../ft_v1/models_cache/models--google--gemma-3-4b-it/snapshots/093f9f388b31de276ce2de164bdc2081324b9767/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"top_k\": 64,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Конфигурация квантования\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                     # Загрузить модель в 8-битном формате\n",
    "    bnb_8bit_quant_type=\"nf8\",             # Тип 8-битного квантования (может быть nf8 или int8)\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16, # Тип данных для вычислений в 8-битном режиме\n",
    "    bnb_8bit_use_double_quant=False        # Использовать ли двойное квантование\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['model']['name'], \n",
    "    cache_dir=CONFIG['model']['cache_dir']\n",
    ")\n",
    "tokenizer.pad_token_id = 0 #Устанавливаем токен для отступа (Используется для добавления до максимальной длинны)\n",
    "tokenizer.padding_side = CONFIG['model']['tokenizer']['padding_size'] #Добавлять до максимальной длинны справа\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model']['name'], \n",
    "    cache_dir=CONFIG['model']['cache_dir'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quant_config,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"cuda:1\" # Автоматическое распределение по доступным GPU\n",
    ")\n",
    "model.use_cache=True\n",
    "\n",
    "#Когда испольуется квант конфиг - модель уже на DEVICE\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a883252-1724-4eff-bfa2-d4f6e7be1778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size val dataloader: 1485\n"
     ]
    }
   ],
   "source": [
    "# Создать даталоадер\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,                # Датасет (например, tokenized_dataset)\n",
    "    batch_size=CONFIG['train']['val_batch'],         # Размер батча\n",
    "    shuffle=False,         # Перемешивать данные\n",
    "    num_workers=0,         # Количество потоков для загрузки\n",
    "    collate_fn=custom_collate_fn,       # Функция для сборки батча\n",
    "    pin_memory=False,      # Копировать данные в CUDA-память\n",
    "    drop_last=False,       # Отбрасывать последний неполный батч\n",
    "    prefetch_factor=None,     # Количество батчей для предварительной загрузки (сколько батчей будет загружено сразу для  ускорения, только в параллельном режиме (когда num_workers > 0)) (None - если не используем)\n",
    "    persistent_workers=False  # Сохранять рабочие потоки между итерациями\n",
    ")\n",
    "\n",
    "print(f\"size val dataloader: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c487b7-73f5-46f9-92f4-7bd3d42a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Нормализует текст: нижний регистр, удаление пробелов и пунктуации\n",
    "    \"\"\"\n",
    "    text = text.lower() # Приводим к нижнему регистру\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # Удаляем пунктуацию\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Удаляем лишние пробелы\n",
    "    return text\n",
    "\n",
    "def compute_exact_match(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Вычисляет Exact Match для одного примера\n",
    "    Принимает 2 строки для сравнения\n",
    "    \"\"\"\n",
    "    return int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "\n",
    "def compute_f1_score(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Вычисляет F1 Score для одного примера\n",
    "    Принимает 2 строки для сравнения\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(ground_truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 and len(truth_tokens) == 0: # Если оба ответа пустые, F1 = 1\n",
    "        return 1.0\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0: # Если один из ответов пустой, F1 = 0\n",
    "        return 0.0\n",
    "    \n",
    "    # Находим общие токены\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    tp = len(common_tokens)\n",
    "    \n",
    "    precision = tp / len(pred_tokens)  # Precision = TP / (TP + FP), где FP = предсказанные токены, не входящие в правильные\n",
    "    recall = tp / len(truth_tokens) # Recall = TP / (TP + FN), где FN = правильные токены, не входящие в предсказанные\n",
    "    \n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_evaluate_metrics_EM_F1(em_all, f1_all):\n",
    "    ''' \n",
    "    Функция для вычисления среднего значения EM и F1\n",
    "    '''\n",
    "    em_all = np.array(em_all)\n",
    "    f1_all = np.array(f1_all)\n",
    "    print(f\"EM: {em_all.mean()}, F1: {f1_all.mean()}\")\n",
    "    return em_all.mean(), f1_all.mean()\n",
    "\n",
    "\n",
    "def evaluate_model_for_metrics(model, tokenizer, dataloader, device, max_new_tokens=32, temp=0.7, logger=None, pfraze_no_answ=\"No answer\"):\n",
    "    \"\"\"Функция для оценки модели по метриками EM, F1 \"\"\"\n",
    "    em_all = []\n",
    "    f1_all = []\n",
    "\n",
    "    if logger:\n",
    "        logger.log(\"Test model. Calculate metrics\")\n",
    "\n",
    "    for batch, item in enumerate(tqdm(dataloader, desc=\"Test model\")):\n",
    "        with autocast(dtype=torch.bfloat16): #Использовать смешанную точность (Работает)\n",
    "            #Генерируем батч\n",
    "            pred = model.generate(\n",
    "                input_ids=item['input_ids'].to(device),\n",
    "                attention_mask=item['attention_mask'].to(device),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temp\n",
    "            )\n",
    "\n",
    "            tensor_shape_1 = item['input_ids'].shape[1]\n",
    "            answers_model_text = tokenizer.batch_decode(pred[:, tensor_shape_1:], skip_special_tokens=True)\n",
    "            # item['answer']\n",
    "\n",
    "            #Лишний цикл, но если внести в циклы которые ниже, то ничего не вычисляет\n",
    "            #только с помощью этого стало нормально определять, если \n",
    "            # answers_model_text = [\"\" if normalize_text(item) == \"no answer\" else item for item in answers_model_text]\n",
    "            \n",
    "            tmp_em = [compute_exact_match(\"\", answer)\n",
    "                      if normalize_text(predict) == \"no answer\" else compute_exact_match(predict, answer)\n",
    "                      for predict, answer in zip(answers_model_text, item['answer'])\n",
    "            ]\n",
    "            tmp_f1 = [compute_f1_score(\"\", answer)\n",
    "                      if normalize_text(predict) == \"no answer\" else compute_f1_score(predict, answer)\n",
    "                      for predict, answer in zip(answers_model_text, item['answer'])\n",
    "             ]\n",
    "            \n",
    "            if logger:\n",
    "                for predict, answer, emtmp, f1tmp in zip(answers_model_text, item['answer'], tmp_em, tmp_f1):\n",
    "                    logger.log(f\"\\nPRED:{predict}\\nANSW:{answer}\\nEM:{emtmp}\\nF1:{f1tmp}\\n\")\n",
    "        \n",
    "        em_all += tmp_em\n",
    "        f1_all += tmp_f1\n",
    "    \n",
    "    em, f1 = calculate_evaluate_metrics_EM_F1(em_all, f1_all)\n",
    "    if logger:\n",
    "        logger.log(f\"em_all_len: {len(em_all)} f1_all_len: {len(f1_all)}\")\n",
    "        logger.log(f\"EM: {em} F1: {f1}\")\n",
    "    return em_all, f1_all, em, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b001a9b-4b36-45cf-9090-eb396ee474ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для отлова логгирования метрик\n",
    "class CustomLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Записываем метрики в лог\n",
    "            self.logger.info(\n",
    "            \"\\n\" + \"\\n\".join([f\"Step {state.global_step}: {key} = {value}\" for key, value in logs.items()]))\n",
    "\n",
    "\n",
    "class CustomLogger:\n",
    "    def __init__(self, log_dir):\n",
    "        self._timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        self._log_filename = f\"{log_dir}/log_run_{self._timestamp}.log\"\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,  # Уровень логирования\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',  # Формат сообщений\n",
    "            handlers=[\n",
    "                logging.FileHandler(self._log_filename),  # Запись в файл\n",
    "                # logging.StreamHandler()  # Вывод в консоль (опционально)\n",
    "            ],\n",
    "            encoding='UTF-8'\n",
    "        )\n",
    "\n",
    "        transformers_logger = logging.getLogger(\"transformers\")\n",
    "        transformers_logger.setLevel(logging.INFO)\n",
    "        file_handler = logging.FileHandler(self._log_filename)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        transformers_logger.addHandler(file_handler)\n",
    "\n",
    "        self._my_logger = logging.getLogger(\"my_custom_logger\") #Перехватить логгер transformers\n",
    "        self._my_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "    def log(self, text_for_log : str):\n",
    "        self._my_logger.info(text_for_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b94d1230-0dae-44be-b26b-b6c5ca6ec6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "PREFIX_PATH_FOR_MODEL_DIRS = f'{CONFIG['logs']['dir']}/train__{time_stamp}__{CONFIG['model']['model_name_log']}'\n",
    "\n",
    "os.makedirs(PREFIX_PATH_FOR_MODEL_DIRS, exist_ok=True)\n",
    "mylogger = CustomLogger(log_dir=PREFIX_PATH_FOR_MODEL_DIRS)\n",
    "\n",
    "mylogger.log(f\"\"\"Start logger\n",
    "--- CONFIGURATE --- \n",
    "{CONFIG}\n",
    "---  --- \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c6ba4-7be2-4f29-b987-285d8eac0a8e",
   "metadata": {},
   "source": [
    "### Загрузка LoRA конфига, который использовался при обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e71f20b-04a9-4f21-8e26-19ab3d99d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint обучения\n",
    "check_path = \"./saved_models/save_state_train_08-06-2025_00-20-25_0.1105502/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ff3f23b-a938-4a35-981b-b5ec0e69b67b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# lora_config = PeftConfig.from_pretrained(check_path)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model = prepare_model_for_kbit_training(model)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# model = get_peft_model(model, lora_config)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/peft/peft_model.py:541\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    533\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    534\u001b[39m         model,\n\u001b[32m    535\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    538\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    539\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    552\u001b[39m missing_keys = [\n\u001b[32m    553\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    554\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/peft/peft_model.py:1272\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[39m\n\u001b[32m   1269\u001b[39m     peft_config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_adapter(adapter_name, peft_config, low_cpu_mem_usage=low_cpu_mem_usage)\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m adapters_weights = \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[32m   1275\u001b[39m ignore_mismatched_sizes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mignore_mismatched_sizes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/peft/utils/save_and_load.py:567\u001b[39m, in \u001b[36mload_peft_weights\u001b[39m\u001b[34m(model_id, device, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    565\u001b[39m         adapters_weights = safe_load_file(filename, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m         adapters_weights = \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    569\u001b[39m     adapters_weights = torch_load(filename, map_location=torch.device(device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/prog/nvenv/lib/python3.12/site-packages/safetensors/torch.py:315\u001b[39m, in \u001b[36mload_file\u001b[39m\u001b[34m(filename, device)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f.keys():\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m         result[k] = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# lora_config = PeftConfig.from_pretrained(check_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, check_path)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "mylogger.log(f\"\"\"\n",
    "model architecture:\n",
    "{str(model)}\n",
    "\n",
    "model congif:\n",
    "{model.config}\n",
    "\n",
    "training parameters:\n",
    "{model.print_trainable_parameters()}\n",
    "\"\"\")\n",
    "print(f\"\\nLoRA model:\\n{str(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce5540-54bd-42f5-aa09-be40dc985080",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_all, f1_all, em, f1 = evaluate_model_for_metrics(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    val_dataloader,\n",
    "    DEVICE,\n",
    "    max_new_tokens=CONFIG['model']['max_new_tokens'],\n",
    "    logger=mylogger,\n",
    "    pfraze_no_answ=\"No answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff62087-f38b-48f4-b6a4-c32c02166a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Оценка тонко настроенной модели:\n",
    "### Какое сохранение использовал, путь:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
